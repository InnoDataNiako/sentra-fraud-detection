"""
√âvaluation des mod√®les de d√©tection de fraude
"""

import pandas as pd
import numpy as np
from typing import Dict, Any, Optional
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    confusion_matrix,
    classification_report,
    precision_recall_curve,
    roc_curve
)
import matplotlib.pyplot as plt
import seaborn as sns
from src.core.logging import get_logger

logger = get_logger(__name__)


class FraudModelEvaluator:
    """√âvaluateur de performance pour mod√®les de d√©tection de fraude"""
    
    def __init__(self):
        self.metrics = {}
        self.y_true = None
        self.y_pred = None
        self.y_proba = None
    
    def evaluate(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        y_proba: Optional[np.ndarray] = None
    ) -> Dict[str, Any]:
        """
        √âvalue les performances du mod√®le
        
        Args:
            y_true: Labels r√©els
            y_pred: Pr√©dictions binaires (0 ou 1)
            y_proba: Probabilit√©s de fraude (optionnel)
            
        Returns:
            Dictionnaire avec toutes les m√©triques
        """
        logger.info("üìä √âvaluation des performances...")
        
        self.y_true = y_true
        self.y_pred = y_pred
        self.y_proba = y_proba
        
        # M√©triques de base
        metrics = {
            'accuracy': accuracy_score(y_true, y_pred),
            'precision': precision_score(y_true, y_pred, zero_division=0),
            'recall': recall_score(y_true, y_pred, zero_division=0),
            'f1_score': f1_score(y_true, y_pred, zero_division=0)
        }
        
        # AUC-ROC si probabilit√©s disponibles
        if y_proba is not None:
            metrics['auc_roc'] = roc_auc_score(y_true, y_proba)
        
        # Matrice de confusion
        cm = confusion_matrix(y_true, y_pred)
        tn, fp, fn, tp = cm.ravel()
        
        metrics.update({
            'true_negatives': int(tn),
            'false_positives': int(fp),
            'false_negatives': int(fn),
            'true_positives': int(tp),
            'total_samples': len(y_true),
            'total_frauds': int(y_true.sum()),
            'total_legitimate': int((y_true == 0).sum())
        })
        
        # M√©triques m√©tier importantes
        metrics['false_positive_rate'] = fp / (fp + tn) if (fp + tn) > 0 else 0
        metrics['false_negative_rate'] = fn / (fn + tp) if (fn + tp) > 0 else 0
        metrics['detection_rate'] = tp / (tp + fn) if (tp + fn) > 0 else 0
        
        # Co√ªt business (exemple avec co√ªts fictifs)
        cost_fp = 10  # Co√ªt d'une fausse alerte (investigation)
        cost_fn = 100  # Co√ªt d'une fraude non d√©tect√©e
        metrics['estimated_cost'] = (fp * cost_fp) + (fn * cost_fn)
        
        self.metrics = metrics
        
        # Afficher r√©sum√©
        self._print_summary()
        
        return metrics
    
    def _print_summary(self):
        """Affiche un r√©sum√© des performances"""
        logger.info("="*70)
        logger.info("üìà R√âSULTATS D'√âVALUATION")
        logger.info("="*70)
        
        m = self.metrics
        
        logger.info(f"\nüéØ M√©triques Principales:")
        logger.info(f"   - Accuracy:  {m['accuracy']:.4f}")
        logger.info(f"   - Precision: {m['precision']:.4f}")
        logger.info(f"   - Recall:    {m['recall']:.4f}")
        logger.info(f"   - F1-Score:  {m['f1_score']:.4f}")
        if 'auc_roc' in m:
            logger.info(f"   - AUC-ROC:   {m['auc_roc']:.4f}")
        
        logger.info(f"\nüìä Matrice de Confusion:")
        logger.info(f"   - True Positives (TP):  {m['true_positives']}")
        logger.info(f"   - True Negatives (TN):  {m['true_negatives']}")
        logger.info(f"   - False Positives (FP): {m['false_positives']}")
        logger.info(f"   - False Negatives (FN): {m['false_negatives']}")
        
        logger.info(f"\nüíº M√©triques M√©tier:")
        logger.info(f"   - Taux Faux Positifs: {m['false_positive_rate']:.4f} ({m['false_positive_rate']*100:.2f}%)")
        logger.info(f"   - Taux Faux N√©gatifs: {m['false_negative_rate']:.4f} ({m['false_negative_rate']*100:.2f}%)")
        logger.info(f"   - Taux D√©tection:     {m['detection_rate']:.4f} ({m['detection_rate']*100:.2f}%)")
        logger.info(f"   - Co√ªt Estim√©:        {m['estimated_cost']:.0f} unit√©s")
        
        logger.info("="*70)
    
    def get_classification_report(self) -> str:
        """
        G√©n√®re un rapport de classification d√©taill√©
        
        Returns:
            Rapport texte
        """
        if self.y_true is None or self.y_pred is None:
            raise ValueError("Appeler evaluate() d'abord")
        
        target_names = ['L√©gitime', 'Fraude']
        report = classification_report(
            self.y_true,
            self.y_pred,
            target_names=target_names,
            digits=4
        )
        
        return report
    
    def plot_confusion_matrix(self, save_path: Optional[str] = None):
        """
        Affiche la matrice de confusion
        
        Args:
            save_path: Chemin pour sauvegarder l'image (optionnel)
        """
        if self.y_true is None or self.y_pred is None:
            raise ValueError("Appeler evaluate() d'abord")
        
        cm = confusion_matrix(self.y_true, self.y_pred)
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(
            cm,
            annot=True,
            fmt='d',
            cmap='Blues',
            xticklabels=['L√©gitime', 'Fraude'],
            yticklabels=['L√©gitime', 'Fraude']
        )
        plt.title('Matrice de Confusion')
        plt.ylabel('Valeur R√©elle')
        plt.xlabel('Pr√©diction')
        
        if save_path:
            plt.savefig(save_path, bbox_inches='tight', dpi=300)
            logger.info(f"‚úÖ Matrice de confusion sauvegard√©e: {save_path}")
        
        plt.close()
    
    def plot_roc_curve(self, save_path: Optional[str] = None):
        """
        Affiche la courbe ROC
        
        Args:
            save_path: Chemin pour sauvegarder l'image (optionnel)
        """
        if self.y_proba is None:
            logger.warning("‚ö†Ô∏è Probabilit√©s non disponibles, impossible de tracer ROC")
            return
        
        fpr, tpr, thresholds = roc_curve(self.y_true, self.y_proba)
        auc = roc_auc_score(self.y_true, self.y_proba)
        
        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.4f})', linewidth=2)
        plt.plot([0, 1], [0, 1], 'k--', label='Random')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('Courbe ROC - D√©tection de Fraude')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        if save_path:
            plt.savefig(save_path, bbox_inches='tight', dpi=300)
            logger.info(f"‚úÖ Courbe ROC sauvegard√©e: {save_path}")
        
        plt.close()
    
    def plot_precision_recall_curve(self, save_path: Optional[str] = None):
        """
        Affiche la courbe Precision-Recall
        
        Args:
            save_path: Chemin pour sauvegarder l'image (optionnel)
        """
        if self.y_proba is None:
            logger.warning("‚ö†Ô∏è Probabilit√©s non disponibles")
            return
        
        precision, recall, thresholds = precision_recall_curve(self.y_true, self.y_proba)
        
        plt.figure(figsize=(8, 6))
        plt.plot(recall, precision, linewidth=2)
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title('Courbe Precision-Recall')
        plt.grid(True, alpha=0.3)
        
        if save_path:
            plt.savefig(save_path, bbox_inches='tight', dpi=300)
            logger.info(f"‚úÖ Courbe P-R sauvegard√©e: {save_path}")
        
        plt.close()
    
    def find_optimal_threshold(self, metric: str = 'f1') -> float:
        """
        Trouve le seuil optimal pour maximiser une m√©trique
        
        Args:
            metric: M√©trique √† optimiser ('f1', 'precision', 'recall')
            
        Returns:
            Seuil optimal
        """
        if self.y_proba is None:
            raise ValueError("Probabilit√©s n√©cessaires")
        
        thresholds = np.arange(0.0, 1.0, 0.01)
        best_threshold = 0.5
        best_score = 0.0
        
        for threshold in thresholds:
            y_pred_temp = (self.y_proba >= threshold).astype(int)
            
            if metric == 'f1':
                score = f1_score(self.y_true, y_pred_temp, zero_division=0)
            elif metric == 'precision':
                score = precision_score(self.y_true, y_pred_temp, zero_division=0)
            elif metric == 'recall':
                score = recall_score(self.y_true, y_pred_temp, zero_division=0)
            else:
                raise ValueError(f"M√©trique inconnue: {metric}")
            
            if score > best_score:
                best_score = score
                best_threshold = threshold
        
        logger.info(f"‚úÖ Seuil optimal ({metric}): {best_threshold:.2f} (score: {best_score:.4f})")
        
        return best_threshold
    
    def compare_models(self, results: Dict[str, Dict[str, Any]]):
        """
        Compare plusieurs mod√®les
        
        Args:
            results: Dict {nom_mod√®le: m√©triques}
        """
        logger.info("="*70)
        logger.info("üîç COMPARAISON DES MOD√àLES")
        logger.info("="*70)
        
        df = pd.DataFrame(results).T
        
        # Afficher tableau comparatif
        logger.info(f"\n{df.to_string()}")
        
        # Identifier le meilleur
        best_model = df['f1_score'].idxmax()
        logger.info(f"\nüèÜ Meilleur mod√®le (F1): {best_model}")
        
        return df
"""
Script d'entraÃ®nement des modÃ¨les de dÃ©tection de fraude
Supporte: Kaggle, SÃ‰NTRA, ou les deux (hybride)
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

import pandas as pd
import argparse
from datetime import datetime
from src.ml.training.trainer import FraudModelTrainer
from src.ml.training.evaluator import FraudModelEvaluator
from src.core.logging import setup_logging, get_logger

setup_logging()
logger = get_logger(__name__)


def train_on_kaggle(output_dir: str = "./data/models/production"):
    """
    EntraÃ®ne un modÃ¨le sur le dataset Kaggle
    
    Args:
        output_dir: Dossier de sortie pour le modÃ¨le
        
    Returns:
        Tuple (trainer, metrics)
    """
    logger.info("="*70)
    logger.info("ğŸ”§ ENTRAÃNEMENT SUR DATASET KAGGLE")
    logger.info("="*70)
    
    # Charger les donnÃ©es
    logger.info("\nğŸ“¥ Chargement des donnÃ©es Kaggle...")
    train_df = pd.read_csv("./data/processed/kaggle_train.csv")
    test_df = pd.read_csv("./data/processed/kaggle_test.csv")
    
    logger.info(f"   - Train: {len(train_df)} transactions")
    logger.info(f"   - Test: {len(test_df)} transactions")
    
    # Initialiser le trainer
    trainer = FraudModelTrainer(
        model_type='random_forest',
        use_smote=True,  # Important pour dataset trÃ¨s dÃ©sÃ©quilibrÃ©
        test_size=0.2
    )
    
    # PrÃ©parer les donnÃ©es (Kaggle = dÃ©jÃ  preprocessÃ©es)
    X_train, y_train = trainer.prepare_data(train_df, is_kaggle=True)
    X_test, y_test = trainer.prepare_data(test_df, is_kaggle=True)
    
    # EntraÃ®nement
    logger.info("\nğŸš€ EntraÃ®nement du modÃ¨le...")
    history = trainer.train(X_train, y_train)
    
    # Ã‰valuation
    logger.info("\nğŸ“Š Ã‰valuation sur test set...")
    evaluator = FraudModelEvaluator()
    
    y_pred = trainer.model.predict(X_test)
    y_proba = trainer.model.get_fraud_probability(X_test)
    
    metrics = evaluator.evaluate(y_test, y_pred, y_proba)
    
    # Sauvegarder le modÃ¨le
    logger.info("\nğŸ’¾ Sauvegarde du modÃ¨le...")
    model_path = trainer.save_model(output_dir)
    
    # Graphiques
    evaluator.plot_confusion_matrix(f"{output_dir}/kaggle_confusion_matrix.png")
    evaluator.plot_roc_curve(f"{output_dir}/kaggle_roc_curve.png")
    
    logger.info("\n" + evaluator.get_classification_report())
    
    return trainer, metrics


def train_on_sentra(output_dir: str = "./data/models/production"):
    """
    EntraÃ®ne un modÃ¨le sur le dataset SÃ‰NTRA
    
    Args:
        output_dir: Dossier de sortie
        
    Returns:
        Tuple (trainer, metrics)
    """
    logger.info("="*70)
    logger.info("ğŸ”§ ENTRAÃNEMENT SUR DATASET SÃ‰NTRA (BCEAO)")
    logger.info("="*70)
    
    # Charger les donnÃ©es
    logger.info("\nğŸ“¥ Chargement des donnÃ©es SÃ‰NTRA...")
    train_df = pd.read_csv("./data/processed/transactions_train.csv")
    test_df = pd.read_csv("./data/processed/transactions_test.csv")
    
    logger.info(f"   - Train: {len(train_df)} transactions")
    logger.info(f"   - Test: {len(test_df)} transactions")
    
    # Initialiser le trainer
    trainer = FraudModelTrainer(
        model_type='random_forest',
        use_smote=False,  # Dataset dÃ©jÃ  plus Ã©quilibrÃ©
        test_size=0.2
    )
    
    # PrÃ©parer les donnÃ©es (SÃ‰NTRA = extraction features nÃ©cessaire)
    X_train, y_train = trainer.prepare_data(train_df, is_kaggle=False)
    X_test, y_test = trainer.prepare_data(test_df, is_kaggle=False)
    
    # EntraÃ®nement
    logger.info("\nğŸš€ EntraÃ®nement du modÃ¨le...")
    history = trainer.train(X_train, y_train)
    
    # Ã‰valuation
    logger.info("\nğŸ“Š Ã‰valuation sur test set...")
    evaluator = FraudModelEvaluator()
    
    y_pred = trainer.model.predict(X_test)
    y_proba = trainer.model.get_fraud_probability(X_test)
    
    metrics = evaluator.evaluate(y_test, y_pred, y_proba)
    
    # Afficher features importantes
    logger.info("\nğŸ” Top 10 Features Importantes:")
    feature_imp = trainer.model.get_feature_importances(10)
    for idx, row in feature_imp.iterrows():
        logger.info(f"   {idx+1}. {row['feature']}: {row['importance']:.4f}")
    
    # Sauvegarder
    logger.info("\nğŸ’¾ Sauvegarde du modÃ¨le...")
    model_path = trainer.save_model(output_dir)
    
    # Graphiques
    evaluator.plot_confusion_matrix(f"{output_dir}/sentra_confusion_matrix.png")
    evaluator.plot_roc_curve(f"{output_dir}/sentra_roc_curve.png")
    
    logger.info("\n" + evaluator.get_classification_report())
    
    return trainer, metrics


def train_hybrid(output_dir: str = "./data/models/production"):
    """
    EntraÃ®ne sur les deux datasets et compare
    
    Args:
        output_dir: Dossier de sortie
        
    Returns:
        Dict avec rÃ©sultats des deux modÃ¨les
    """
    logger.info("="*70)
    logger.info("ğŸš€ ENTRAÃNEMENT HYBRIDE (KAGGLE + SÃ‰NTRA)")
    logger.info("="*70)
    
    # EntraÃ®ner sur Kaggle
    logger.info("\n" + "="*70)
    logger.info("PARTIE 1/3 : ENTRAÃNEMENT KAGGLE")
    logger.info("="*70)
    trainer_kaggle, metrics_kaggle = train_on_kaggle(output_dir)
    
    # EntraÃ®ner sur SÃ‰NTRA
    logger.info("\n" + "="*70)
    logger.info("PARTIE 2/3 : ENTRAÃNEMENT SÃ‰NTRA")
    logger.info("="*70)
    trainer_sentra, metrics_sentra = train_on_sentra(output_dir)
    
    # Validation croisÃ©e : tester modÃ¨le Kaggle sur donnÃ©es SÃ‰NTRA
    logger.info("\n" + "="*70)
    logger.info("PARTIE 3/3 : VALIDATION CROISÃ‰E")
    logger.info("="*70)
    
    logger.info("\nğŸ”„ Test du modÃ¨le Kaggle sur donnÃ©es SÃ‰NTRA...")
    test_sentra = pd.read_csv("./data/processed/transactions_test.csv")
    
    # PrÃ©parer donnÃ©es SÃ‰NTRA
    X_sentra_test, y_sentra_test = trainer_sentra.prepare_data(test_sentra, is_kaggle=False)
    
    # Attention: modÃ¨le Kaggle attend features Kaggle
    # On ne peut pas le tester directement sur features SÃ‰NTRA (diffÃ©rentes)
    logger.info("âš ï¸  Les features sont diffÃ©rentes - validation croisÃ©e directe impossible")
    logger.info("ğŸ’¡ Utiliser transfer learning ou rÃ©entraÃ®nement serait nÃ©cessaire")
    
    # Comparaison finale
    logger.info("\n" + "="*70)
    logger.info("ğŸ“Š COMPARAISON FINALE")
    logger.info("="*70)
    
    evaluator = FraudModelEvaluator()
    comparison = {
        'Kaggle': metrics_kaggle,
        'SÃ‰NTRA': metrics_sentra
    }
    
    df_comparison = evaluator.compare_models(comparison)
    
    # Sauvegarder la comparaison
    df_comparison.to_csv(f"{output_dir}/model_comparison.csv")
    logger.info(f"\nâœ… Comparaison sauvegardÃ©e: {output_dir}/model_comparison.csv")
    
    return {
        'kaggle': {'trainer': trainer_kaggle, 'metrics': metrics_kaggle},
        'sentra': {'trainer': trainer_sentra, 'metrics': metrics_sentra},
        'comparison': df_comparison
    }


def main():
    """Fonction principale"""
    
    parser = argparse.ArgumentParser(description='EntraÃ®ner un modÃ¨le de dÃ©tection de fraude')
    parser.add_argument(
        '--dataset',
        type=str,
        choices=['kaggle', 'sentra', 'both'],
        default='sentra',
        help='Dataset Ã  utiliser (kaggle, sentra, ou both)'
    )
    parser.add_argument(
        '--output',
        type=str,
        default='./data/models/production',
        help='Dossier de sortie pour les modÃ¨les'
    )
    
    args = parser.parse_args()
    
    logger.info("="*70)
    logger.info("ğŸš€ SÃ‰NTRA - ENTRAÃNEMENT MODÃˆLES ML")
    logger.info(f"   Dataset: {args.dataset}")
    logger.info(f"   Output: {args.output}")
    logger.info("="*70)
    
    # CrÃ©er dossier output
    Path(args.output).mkdir(parents=True, exist_ok=True)
    
    # EntraÃ®ner selon le choix
    start_time = datetime.now()
    
    if args.dataset == 'kaggle':
        trainer, metrics = train_on_kaggle(args.output)
    elif args.dataset == 'sentra':
        trainer, metrics = train_on_sentra(args.output)
    elif args.dataset == 'both':
        results = train_hybrid(args.output)
    
    total_time = (datetime.now() - start_time).total_seconds()
    
    logger.info("\n" + "="*70)
    logger.info("âœ… ENTRAÃNEMENT TERMINÃ‰")
    logger.info("="*70)
    logger.info(f"   Temps total: {total_time:.2f}s")
    logger.info(f"   ModÃ¨les sauvegardÃ©s dans: {args.output}")
    logger.info("")
    logger.info("ğŸ“ Prochaines Ã©tapes:")
    logger.info("   1. Tester l'API: uvicorn src.api.main:app --reload")
    logger.info("   2. Faire une prÃ©diction test")
    logger.info("   3. DÃ©ployer le modÃ¨le en production")


if __name__ == "__main__":
    main()